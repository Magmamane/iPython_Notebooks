{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных и логистическая регрессия для задачи бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании вам будет предложено ознакомиться с основными техниками предобработки данных, а так же применить их для обучения модели логистической регрессии. Ответ потребуется загрузить в соответствующую форму в виде 6 текстовых файлов.\n",
    "\n",
    "Для выполнения задания требуется Python версии 2.7, а также актуальные версии библиотек:\n",
    "- NumPy: 1.10.4 и выше\n",
    "- Pandas: 0.17.1 и выше\n",
    "- Scikit-learn: 0.17 и выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: по 38 признакам, связанных с заявкой на грант (область исследований учёных, информация по их академическому бэкграунду, размер гранта, область, в которой он выдаётся) предсказать, будет ли заявка принята. Датасет включает в себя информацию по 6000 заявкам на гранты, которые были поданы в университете Мельбурна в период с 2004 по 2008 год.\n",
    "\n",
    "Полную версию данных с большим количеством признаков можно найти на https://www.kaggle.com/c/unimelb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Grant.Status Sponsor.Code Grant.Category.Code  \\\n",
      "0             1          21A                 50A   \n",
      "1             1           4D                 10A   \n",
      "2             0          NaN                 NaN   \n",
      "3             0          51C                 20C   \n",
      "4             0          24D                 30B   \n",
      "\n",
      "  Contract.Value.Band...see.note.A  RFCD.Code.1  RFCD.Percentage.1  \\\n",
      "0                               A      230202.0               50.0   \n",
      "1                               D      320801.0              100.0   \n",
      "2                              NaN     320602.0               50.0   \n",
      "3                               A      291503.0               60.0   \n",
      "4                              NaN     380107.0              100.0   \n",
      "\n",
      "   RFCD.Code.2  RFCD.Percentage.2  RFCD.Code.3  RFCD.Percentage.3 ...   \\\n",
      "0     230203.0               30.0     230204.0               20.0 ...    \n",
      "1          0.0                0.0          0.0                0.0 ...    \n",
      "2     321004.0               30.0     321015.0               20.0 ...    \n",
      "3     321402.0               40.0          0.0                0.0 ...    \n",
      "4          0.0                0.0          0.0                0.0 ...    \n",
      "\n",
      "   Dept.No..1  Faculty.No..1  With.PHD.1  \\\n",
      "0      3098.0           31.0        Yes    \n",
      "1      2553.0           25.0        Yes    \n",
      "2      2813.0           25.0         NaN   \n",
      "3      2553.0           25.0         NaN   \n",
      "4      2923.0           25.0         NaN   \n",
      "\n",
      "   No..of.Years.in.Uni.at.Time.of.Grant.1  Number.of.Successful.Grant.1  \\\n",
      "0                                >=0 to 5                           2.0   \n",
      "1                                >=0 to 5                           3.0   \n",
      "2                             Less than 0                           1.0   \n",
      "3                            more than 15                           2.0   \n",
      "4                             Less than 0                           0.0   \n",
      "\n",
      "   Number.of.Unsuccessful.Grant.1  A..1  A.1  B.1  C.1  \n",
      "0                             0.0   0.0  4.0  2.0  0.0  \n",
      "1                             1.0   0.0  2.0  0.0  0.0  \n",
      "2                             5.0   0.0  7.0  2.0  0.0  \n",
      "3                             1.0   5.0  6.0  9.0  1.0  \n",
      "4                             2.0   0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.shape\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим из датасета целевую переменную Grant.Status и обозначим её за y\n",
    "Теперь X обозначает обучающую выборку, y - ответы на ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Grant.Status', 1)\n",
    "y = data['Grant.Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория по логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После осознания того, какую именно задачу требуется решить на этих данных, следующим шагом при реальном анализе был бы подбор подходящего метода. В данном задании выбор метода было произведён за вас, это логистическая регрессия. Кратко напомним вам используемую модель.\n",
    "\n",
    "Логистическая регрессия предсказывает вероятности принадлежности объекта к каждому классу. Сумма ответов логистической регрессии на одном объекте для всех классов равна единице.\n",
    "\n",
    "$$ \\sum_{k=1}^K \\pi_{ik} = 1, \\quad \\pi_k \\equiv P\\,(y_i = k \\mid x_i, \\theta), $$\n",
    "\n",
    "где:\n",
    "- $\\pi_{ik}$ - вероятность принадлежности объекта $x_i$ из выборки $X$ к классу $k$\n",
    "- $\\theta$ - внутренние параметры алгоритма, которые настраиваются в процессе обучения, в случае логистической регрессии - $w, b$\n",
    "\n",
    "Из этого свойства модели в случае бинарной классификации требуется вычислить лишь вероятность принадлежности объекта к одному из классов (вторая вычисляется из условия нормировки вероятностей). Эта вероятность вычисляется, используя логистическую функцию:\n",
    "\n",
    "$$ P\\,(y_i = 1 \\mid x_i, \\theta) = \\frac{1}{1 + \\exp(-w^T x_i-b)} $$\n",
    "\n",
    "Параметры $w$ и $b$ находятся, как решения следующей задачи оптимизации (указаны функционалы с L1 и L2 регуляризацией, с которыми вы познакомились в предыдущих заданиях):\n",
    "\n",
    "L2-regularization:\n",
    "\n",
    "$$ Q(X, y, \\theta) = \\frac{1}{2} w^T w + C \\sum_{i=1}^l \\log ( 1 + \\exp(-y_i (w^T x_i + b ) ) ) \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "L1-regularization:\n",
    "\n",
    "$$ Q(X, y, \\theta) = \\sum_{d=1}^D |w_d| + C \\sum_{i=1}^l \\log ( 1 + \\exp(-y_i (w^T x_i + b ) ) ) \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "$C$ - это стандартный гиперпараметр модели, который регулирует то, насколько сильно мы позволяем модели подстраиваться под данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из свойств данной модели следует, что:\n",
    "- все $X$ должны быть числовыми данными (в случае наличия среди них категорий, их требуется некоторым способом преобразовать в вещественные числа)\n",
    "- среди $X$ не должно быть пропущенных значений (т.е. все пропущенные значения перед применением модели следует каким-то образом заполнить)\n",
    "\n",
    "Поэтому базовым этапом в предобработке любого датасета для логистической регрессии будет кодирование категориальных признаков, а так же удаление или интерпретация пропущенных значений (при наличии того или другого)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grant.Status</th>\n",
       "      <th>Sponsor.Code</th>\n",
       "      <th>Grant.Category.Code</th>\n",
       "      <th>Contract.Value.Band...see.note.A</th>\n",
       "      <th>RFCD.Code.1</th>\n",
       "      <th>RFCD.Percentage.1</th>\n",
       "      <th>RFCD.Code.2</th>\n",
       "      <th>RFCD.Percentage.2</th>\n",
       "      <th>RFCD.Code.3</th>\n",
       "      <th>RFCD.Percentage.3</th>\n",
       "      <th>...</th>\n",
       "      <th>Dept.No..1</th>\n",
       "      <th>Faculty.No..1</th>\n",
       "      <th>With.PHD.1</th>\n",
       "      <th>No..of.Years.in.Uni.at.Time.of.Grant.1</th>\n",
       "      <th>Number.of.Successful.Grant.1</th>\n",
       "      <th>Number.of.Unsuccessful.Grant.1</th>\n",
       "      <th>A..1</th>\n",
       "      <th>A.1</th>\n",
       "      <th>B.1</th>\n",
       "      <th>C.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21A</td>\n",
       "      <td>50A</td>\n",
       "      <td>A</td>\n",
       "      <td>230202.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>230203.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>230204.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3098.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;=0 to 5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4D</td>\n",
       "      <td>10A</td>\n",
       "      <td>D</td>\n",
       "      <td>320801.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2553.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;=0 to 5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320602.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>321004.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>321015.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2813.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Less than 0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51C</td>\n",
       "      <td>20C</td>\n",
       "      <td>A</td>\n",
       "      <td>291503.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>321402.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2553.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>more than 15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>24D</td>\n",
       "      <td>30B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>380107.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2923.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Less than 0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Grant.Status Sponsor.Code Grant.Category.Code  \\\n",
       "0             1          21A                 50A   \n",
       "1             1           4D                 10A   \n",
       "2             0          NaN                 NaN   \n",
       "3             0          51C                 20C   \n",
       "4             0          24D                 30B   \n",
       "\n",
       "  Contract.Value.Band...see.note.A  RFCD.Code.1  RFCD.Percentage.1  \\\n",
       "0                               A      230202.0               50.0   \n",
       "1                               D      320801.0              100.0   \n",
       "2                              NaN     320602.0               50.0   \n",
       "3                               A      291503.0               60.0   \n",
       "4                              NaN     380107.0              100.0   \n",
       "\n",
       "   RFCD.Code.2  RFCD.Percentage.2  RFCD.Code.3  RFCD.Percentage.3 ...   \\\n",
       "0     230203.0               30.0     230204.0               20.0 ...    \n",
       "1          0.0                0.0          0.0                0.0 ...    \n",
       "2     321004.0               30.0     321015.0               20.0 ...    \n",
       "3     321402.0               40.0          0.0                0.0 ...    \n",
       "4          0.0                0.0          0.0                0.0 ...    \n",
       "\n",
       "   Dept.No..1  Faculty.No..1  With.PHD.1  \\\n",
       "0      3098.0           31.0        Yes    \n",
       "1      2553.0           25.0        Yes    \n",
       "2      2813.0           25.0         NaN   \n",
       "3      2553.0           25.0         NaN   \n",
       "4      2923.0           25.0         NaN   \n",
       "\n",
       "   No..of.Years.in.Uni.at.Time.of.Grant.1  Number.of.Successful.Grant.1  \\\n",
       "0                                >=0 to 5                           2.0   \n",
       "1                                >=0 to 5                           3.0   \n",
       "2                             Less than 0                           1.0   \n",
       "3                            more than 15                           2.0   \n",
       "4                             Less than 0                           0.0   \n",
       "\n",
       "   Number.of.Unsuccessful.Grant.1  A..1  A.1  B.1  C.1  \n",
       "0                             0.0   0.0  4.0  2.0  0.0  \n",
       "1                             1.0   0.0  2.0  0.0  0.0  \n",
       "2                             5.0   0.0  7.0  2.0  0.0  \n",
       "3                             1.0   5.0  6.0  9.0  1.0  \n",
       "4                             2.0   0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в датасете есть как числовые, так и категориальные признаки. Получим списки их названий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['RFCD.Percentage.1', 'RFCD.Percentage.2', 'RFCD.Percentage.3', \n",
    "                'RFCD.Percentage.4', 'RFCD.Percentage.5',\n",
    "                'SEO.Percentage.1', 'SEO.Percentage.2', 'SEO.Percentage.3',\n",
    "                'SEO.Percentage.4', 'SEO.Percentage.5',\n",
    "                'Year.of.Birth.1', 'Number.of.Successful.Grant.1', 'Number.of.Unsuccessful.Grant.1']\n",
    "categorical_cols = list(set(X.columns.values.tolist()) - set(numeric_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в нём присутствуют пропущенные значения. Очевидны решением будет исключение всех данных, у которых пропущено хотя бы одно значение. Сделаем это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 39)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что тогда мы выбросим почти все данные, и такой метод решения в данном случае не сработает.\n",
    "\n",
    "Пропущенные значения можно так же интерпретировать, для этого существует несколько способов, они различаются для категориальных и вещественных признаков.\n",
    "\n",
    "Для вещественных признаков:\n",
    "- заменить на 0 (данный признак давать вклад в предсказание для данного объекта не будет)\n",
    "- заменить на среднее (каждый пропущенный признак будет давать такой же вклад, как и среднее значение признака на датасете)\n",
    "\n",
    "Для категориальных:\n",
    "- интерпретировать пропущенное значение, как ещё одну категорию (данный способ является самым естественным, так как в случае категорий у нас есть уникальная возможность не потерять информацию о наличии пропущенных значений; обратите внимание, что в случае вещественных признаков данная информация неизбежно теряется)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Задание 0. Обработка пропущенных значений.\n",
    "1. Заполните пропущенные вещественные значения в X нулями и средними по столбцам, назовите полученные датафреймы X_real_zeros и X_real_mean соответственно. Для подсчёта средних используйте описанную ниже функцию calculate_means, которой требуется передать на вход вешественные признаки из исходного датафрейма.\n",
    "2. Все категориальные признаки в X преобразуйте в строки, пропущенные значения требуется также преобразовать в какие-либо строки, которые не являются категориями (например, 'NA'), полученный датафрейм назовите X_cat.\n",
    "\n",
    "Для объединения выборок здесь и далее в задании рекомендуется использовать функции\n",
    "\n",
    "    np.hstack(...)\n",
    "    np.vstack(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_means(numeric_data):\n",
    "    means = np.zeros(numeric_data.shape[1])\n",
    "    for j in range(numeric_data.shape[1]):\n",
    "        to_sum = numeric_data.iloc[:,j]\n",
    "        indices = np.nonzero(~numeric_data.iloc[:,j].isnull())[0]\n",
    "        correction = np.amax(to_sum[indices])\n",
    "        to_sum /= correction\n",
    "        for i in indices:\n",
    "            means[j] += to_sum[i]\n",
    "        means[j] /= indices.size\n",
    "        means[j] *= correction\n",
    "    return pd.Series(means, numeric_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = calculate_means(X[numeric_cols])\n",
    "X_real_mean = X[numeric_cols].copy()\n",
    "X_real_zeros = X[numeric_cols].copy()\n",
    "for column in numeric_cols:\n",
    "    X_real_mean[column].fillna(means[column], inplace=True)\n",
    "    X_real_zeros[column].fillna(0, inplace=True)\n",
    "\n",
    "X_cat = X[categorical_cols].copy()\n",
    "X_cat = X_cat[categorical_cols].fillna('NA').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RFCD.Percentage.1</th>\n",
       "      <th>RFCD.Percentage.2</th>\n",
       "      <th>RFCD.Percentage.3</th>\n",
       "      <th>RFCD.Percentage.4</th>\n",
       "      <th>RFCD.Percentage.5</th>\n",
       "      <th>SEO.Percentage.1</th>\n",
       "      <th>SEO.Percentage.2</th>\n",
       "      <th>SEO.Percentage.3</th>\n",
       "      <th>SEO.Percentage.4</th>\n",
       "      <th>SEO.Percentage.5</th>\n",
       "      <th>Year.of.Birth.1</th>\n",
       "      <th>Number.of.Successful.Grant.1</th>\n",
       "      <th>Number.of.Unsuccessful.Grant.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1962.449849</td>\n",
       "      <td>1.177849</td>\n",
       "      <td>2.097977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>33.00000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1962.449849</td>\n",
       "      <td>1.177849</td>\n",
       "      <td>2.097977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>74.832348</td>\n",
       "      <td>17.677593</td>\n",
       "      <td>6.933011</td>\n",
       "      <td>0.437937</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>71.48324</td>\n",
       "      <td>20.64688</td>\n",
       "      <td>6.926704</td>\n",
       "      <td>0.730804</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1962.449849</td>\n",
       "      <td>1.177849</td>\n",
       "      <td>2.097977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5977</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1962.449849</td>\n",
       "      <td>1.177849</td>\n",
       "      <td>2.097977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5978</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5980</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5981</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>74.832348</td>\n",
       "      <td>17.677593</td>\n",
       "      <td>6.933011</td>\n",
       "      <td>0.437937</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>71.48324</td>\n",
       "      <td>20.64688</td>\n",
       "      <td>6.926704</td>\n",
       "      <td>0.730804</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1980.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1962.449849</td>\n",
       "      <td>1.177849</td>\n",
       "      <td>2.097977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>74.832348</td>\n",
       "      <td>17.677593</td>\n",
       "      <td>6.933011</td>\n",
       "      <td>0.437937</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>71.48324</td>\n",
       "      <td>20.64688</td>\n",
       "      <td>6.926704</td>\n",
       "      <td>0.730804</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5990</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5991</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5992</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5993</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>74.832348</td>\n",
       "      <td>17.677593</td>\n",
       "      <td>6.933011</td>\n",
       "      <td>0.437937</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>71.48324</td>\n",
       "      <td>20.64688</td>\n",
       "      <td>6.926704</td>\n",
       "      <td>0.730804</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>74.832348</td>\n",
       "      <td>17.677593</td>\n",
       "      <td>6.933011</td>\n",
       "      <td>0.437937</td>\n",
       "      <td>0.119112</td>\n",
       "      <td>71.48324</td>\n",
       "      <td>20.64688</td>\n",
       "      <td>6.926704</td>\n",
       "      <td>0.730804</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RFCD.Percentage.1  RFCD.Percentage.2  RFCD.Percentage.3  \\\n",
       "0             50.000000          30.000000          20.000000   \n",
       "1            100.000000           0.000000           0.000000   \n",
       "2             50.000000          30.000000          20.000000   \n",
       "3             60.000000          40.000000           0.000000   \n",
       "4            100.000000           0.000000           0.000000   \n",
       "5            100.000000           0.000000           0.000000   \n",
       "6             40.000000          30.000000          30.000000   \n",
       "7             40.000000          30.000000          30.000000   \n",
       "8            100.000000           0.000000           0.000000   \n",
       "9            100.000000           0.000000           0.000000   \n",
       "10           100.000000           0.000000           0.000000   \n",
       "11           100.000000           0.000000           0.000000   \n",
       "12           100.000000           0.000000           0.000000   \n",
       "13           100.000000           0.000000           0.000000   \n",
       "14           100.000000           0.000000           0.000000   \n",
       "15            80.000000          10.000000          10.000000   \n",
       "16           100.000000           0.000000           0.000000   \n",
       "17            20.000000          80.000000           0.000000   \n",
       "18           100.000000           0.000000           0.000000   \n",
       "19           100.000000           0.000000           0.000000   \n",
       "20            50.000000          50.000000           0.000000   \n",
       "21           100.000000           0.000000           0.000000   \n",
       "22            50.000000          50.000000           0.000000   \n",
       "23            70.000000          30.000000           0.000000   \n",
       "24           100.000000           0.000000           0.000000   \n",
       "25            80.000000          10.000000          10.000000   \n",
       "26            50.000000          50.000000           0.000000   \n",
       "27            70.000000          10.000000          10.000000   \n",
       "28            74.832348          17.677593           6.933011   \n",
       "29            50.000000          50.000000           0.000000   \n",
       "...                 ...                ...                ...   \n",
       "5970          50.000000          50.000000           0.000000   \n",
       "5971          50.000000          50.000000           0.000000   \n",
       "5972         100.000000           0.000000           0.000000   \n",
       "5973         100.000000           0.000000           0.000000   \n",
       "5974         100.000000           0.000000           0.000000   \n",
       "5975         100.000000           0.000000           0.000000   \n",
       "5976          50.000000          50.000000           0.000000   \n",
       "5977         100.000000           0.000000           0.000000   \n",
       "5978          50.000000          30.000000          20.000000   \n",
       "5979          40.000000          30.000000          30.000000   \n",
       "5980          30.000000          50.000000          20.000000   \n",
       "5981         100.000000           0.000000           0.000000   \n",
       "5982         100.000000           0.000000           0.000000   \n",
       "5983          50.000000          50.000000           0.000000   \n",
       "5984         100.000000           0.000000           0.000000   \n",
       "5985          74.832348          17.677593           6.933011   \n",
       "5986         100.000000           0.000000           0.000000   \n",
       "5987         100.000000           0.000000           0.000000   \n",
       "5988          74.832348          17.677593           6.933011   \n",
       "5989          50.000000          50.000000           0.000000   \n",
       "5990         100.000000           0.000000           0.000000   \n",
       "5991          40.000000          30.000000          30.000000   \n",
       "5992         100.000000           0.000000           0.000000   \n",
       "5993          75.000000          20.000000           5.000000   \n",
       "5994         100.000000           0.000000           0.000000   \n",
       "5995          50.000000          30.000000          20.000000   \n",
       "5996          74.832348          17.677593           6.933011   \n",
       "5997          40.000000          40.000000          20.000000   \n",
       "5998          74.832348          17.677593           6.933011   \n",
       "5999         100.000000           0.000000           0.000000   \n",
       "\n",
       "      RFCD.Percentage.4  RFCD.Percentage.5  SEO.Percentage.1  \\\n",
       "0              0.000000           0.000000         100.00000   \n",
       "1              0.000000           0.000000         100.00000   \n",
       "2              0.000000           0.000000          60.00000   \n",
       "3              0.000000           0.000000          60.00000   \n",
       "4              0.000000           0.000000          50.00000   \n",
       "5              0.000000           0.000000          50.00000   \n",
       "6              0.000000           0.000000          40.00000   \n",
       "7              0.000000           0.000000          40.00000   \n",
       "8              0.000000           0.000000         100.00000   \n",
       "9              0.000000           0.000000         100.00000   \n",
       "10             0.000000           0.000000         100.00000   \n",
       "11             0.000000           0.000000         100.00000   \n",
       "12             0.000000           0.000000          40.00000   \n",
       "13             0.000000           0.000000          15.00000   \n",
       "14             0.000000           0.000000         100.00000   \n",
       "15             0.000000           0.000000          90.00000   \n",
       "16             0.000000           0.000000         100.00000   \n",
       "17             0.000000           0.000000         100.00000   \n",
       "18             0.000000           0.000000         100.00000   \n",
       "19             0.000000           0.000000          34.00000   \n",
       "20             0.000000           0.000000          60.00000   \n",
       "21             0.000000           0.000000         100.00000   \n",
       "22             0.000000           0.000000          50.00000   \n",
       "23             0.000000           0.000000          50.00000   \n",
       "24             0.000000           0.000000          50.00000   \n",
       "25             0.000000           0.000000          50.00000   \n",
       "26             0.000000           0.000000         100.00000   \n",
       "27            10.000000           0.000000          50.00000   \n",
       "28             0.437937           0.119112          71.48324   \n",
       "29             0.000000           0.000000          50.00000   \n",
       "...                 ...                ...               ...   \n",
       "5970           0.000000           0.000000         100.00000   \n",
       "5971           0.000000           0.000000          40.00000   \n",
       "5972           0.000000           0.000000          10.00000   \n",
       "5973           0.000000           0.000000         100.00000   \n",
       "5974           0.000000           0.000000         100.00000   \n",
       "5975           0.000000           0.000000          50.00000   \n",
       "5976           0.000000           0.000000          60.00000   \n",
       "5977           0.000000           0.000000          30.00000   \n",
       "5978           0.000000           0.000000         100.00000   \n",
       "5979           0.000000           0.000000          30.00000   \n",
       "5980           0.000000           0.000000          20.00000   \n",
       "5981           0.000000           0.000000          25.00000   \n",
       "5982           0.000000           0.000000          60.00000   \n",
       "5983           0.000000           0.000000          25.00000   \n",
       "5984           0.000000           0.000000          40.00000   \n",
       "5985           0.437937           0.119112          71.48324   \n",
       "5986           0.000000           0.000000          25.00000   \n",
       "5987           0.000000           0.000000          50.00000   \n",
       "5988           0.437937           0.119112          71.48324   \n",
       "5989           0.000000           0.000000          60.00000   \n",
       "5990           0.000000           0.000000         100.00000   \n",
       "5991           0.000000           0.000000          50.00000   \n",
       "5992           0.000000           0.000000         100.00000   \n",
       "5993           0.000000           0.000000          75.00000   \n",
       "5994           0.000000           0.000000          60.00000   \n",
       "5995           0.000000           0.000000          50.00000   \n",
       "5996           0.437937           0.119112          71.48324   \n",
       "5997           0.000000           0.000000          40.00000   \n",
       "5998           0.437937           0.119112          71.48324   \n",
       "5999           0.000000           0.000000          95.00000   \n",
       "\n",
       "      SEO.Percentage.2  SEO.Percentage.3  SEO.Percentage.4  SEO.Percentage.5  \\\n",
       "0              0.00000          0.000000          0.000000          0.000000   \n",
       "1              0.00000          0.000000          0.000000          0.000000   \n",
       "2             20.00000         20.000000          0.000000          0.000000   \n",
       "3             40.00000          0.000000          0.000000          0.000000   \n",
       "4             50.00000          0.000000          0.000000          0.000000   \n",
       "5             50.00000          0.000000          0.000000          0.000000   \n",
       "6             30.00000         30.000000          0.000000          0.000000   \n",
       "7             30.00000         30.000000          0.000000          0.000000   \n",
       "8              0.00000          0.000000          0.000000          0.000000   \n",
       "9              0.00000          0.000000          0.000000          0.000000   \n",
       "10             0.00000          0.000000          0.000000          0.000000   \n",
       "11             0.00000          0.000000          0.000000          0.000000   \n",
       "12            60.00000          0.000000          0.000000          0.000000   \n",
       "13            50.00000         15.000000         20.000000          0.000000   \n",
       "14             0.00000          0.000000          0.000000          0.000000   \n",
       "15            10.00000          0.000000          0.000000          0.000000   \n",
       "16             0.00000          0.000000          0.000000          0.000000   \n",
       "17             0.00000          0.000000          0.000000          0.000000   \n",
       "18             0.00000          0.000000          0.000000          0.000000   \n",
       "19            33.00000         33.000000          0.000000          0.000000   \n",
       "20            20.00000         20.000000          0.000000          0.000000   \n",
       "21             0.00000          0.000000          0.000000          0.000000   \n",
       "22            50.00000          0.000000          0.000000          0.000000   \n",
       "23            50.00000          0.000000          0.000000          0.000000   \n",
       "24            50.00000          0.000000          0.000000          0.000000   \n",
       "25            50.00000          0.000000          0.000000          0.000000   \n",
       "26             0.00000          0.000000          0.000000          0.000000   \n",
       "27            50.00000          0.000000          0.000000          0.000000   \n",
       "28            20.64688          6.926704          0.730804          0.212192   \n",
       "29            50.00000          0.000000          0.000000          0.000000   \n",
       "...                ...               ...               ...               ...   \n",
       "5970           0.00000          0.000000          0.000000          0.000000   \n",
       "5971          60.00000          0.000000          0.000000          0.000000   \n",
       "5972          45.00000         45.000000          0.000000          0.000000   \n",
       "5973           0.00000          0.000000          0.000000          0.000000   \n",
       "5974           0.00000          0.000000          0.000000          0.000000   \n",
       "5975          50.00000          0.000000          0.000000          0.000000   \n",
       "5976          40.00000          0.000000          0.000000          0.000000   \n",
       "5977          30.00000         20.000000         20.000000          0.000000   \n",
       "5978           0.00000          0.000000          0.000000          0.000000   \n",
       "5979          30.00000         40.000000          0.000000          0.000000   \n",
       "5980          60.00000         20.000000          0.000000          0.000000   \n",
       "5981          25.00000         25.000000         25.000000          0.000000   \n",
       "5982          40.00000          0.000000          0.000000          0.000000   \n",
       "5983          25.00000         25.000000         25.000000          0.000000   \n",
       "5984          30.00000         30.000000          0.000000          0.000000   \n",
       "5985          20.64688          6.926704          0.730804          0.212192   \n",
       "5986          25.00000         25.000000         25.000000          0.000000   \n",
       "5987          15.00000         15.000000         20.000000          0.000000   \n",
       "5988          20.64688          6.926704          0.730804          0.212192   \n",
       "5989          30.00000         10.000000          0.000000          0.000000   \n",
       "5990           0.00000          0.000000          0.000000          0.000000   \n",
       "5991          50.00000          0.000000          0.000000          0.000000   \n",
       "5992           0.00000          0.000000          0.000000          0.000000   \n",
       "5993          20.00000          5.000000          0.000000          0.000000   \n",
       "5994          25.00000         15.000000          0.000000          0.000000   \n",
       "5995          30.00000         20.000000          0.000000          0.000000   \n",
       "5996          20.64688          6.926704          0.730804          0.212192   \n",
       "5997          40.00000         20.000000          0.000000          0.000000   \n",
       "5998          20.64688          6.926704          0.730804          0.212192   \n",
       "5999           5.00000          0.000000          0.000000          0.000000   \n",
       "\n",
       "      Year.of.Birth.1  Number.of.Successful.Grant.1  \\\n",
       "0         1965.000000                      2.000000   \n",
       "1         1965.000000                      3.000000   \n",
       "2         1955.000000                      1.000000   \n",
       "3         1950.000000                      2.000000   \n",
       "4         1970.000000                      0.000000   \n",
       "5         1960.000000                      0.000000   \n",
       "6         1950.000000                      1.000000   \n",
       "7         1955.000000                      0.000000   \n",
       "8         1962.449849                      1.177849   \n",
       "9         1945.000000                      3.000000   \n",
       "10        1980.000000                      1.000000   \n",
       "11        1945.000000                      0.000000   \n",
       "12        1970.000000                      4.000000   \n",
       "13        1960.000000                      0.000000   \n",
       "14        1945.000000                      0.000000   \n",
       "15        1940.000000                      0.000000   \n",
       "16        1945.000000                      2.000000   \n",
       "17        1965.000000                      0.000000   \n",
       "18        1945.000000                      0.000000   \n",
       "19        1965.000000                      0.000000   \n",
       "20        1950.000000                      2.000000   \n",
       "21        1950.000000                      1.000000   \n",
       "22        1962.449849                      1.177849   \n",
       "23        1950.000000                      0.000000   \n",
       "24        1970.000000                      1.000000   \n",
       "25        1960.000000                      0.000000   \n",
       "26        1975.000000                      1.000000   \n",
       "27        1960.000000                      0.000000   \n",
       "28        1955.000000                      0.000000   \n",
       "29        1965.000000                      1.000000   \n",
       "...               ...                           ...   \n",
       "5970      1975.000000                      0.000000   \n",
       "5971      1975.000000                      0.000000   \n",
       "5972      1965.000000                      0.000000   \n",
       "5973      1960.000000                      0.000000   \n",
       "5974      1950.000000                      0.000000   \n",
       "5975      1962.449849                      1.177849   \n",
       "5976      1945.000000                      2.000000   \n",
       "5977      1962.449849                      1.177849   \n",
       "5978      1975.000000                      1.000000   \n",
       "5979      1960.000000                      3.000000   \n",
       "5980      1975.000000                      1.000000   \n",
       "5981      1960.000000                      1.000000   \n",
       "5982      1975.000000                      4.000000   \n",
       "5983      1955.000000                      0.000000   \n",
       "5984      1970.000000                      1.000000   \n",
       "5985      1970.000000                      1.000000   \n",
       "5986      1980.000000                      0.000000   \n",
       "5987      1962.449849                      1.177849   \n",
       "5988      1965.000000                      0.000000   \n",
       "5989      1945.000000                      0.000000   \n",
       "5990      1955.000000                      1.000000   \n",
       "5991      1965.000000                      1.000000   \n",
       "5992      1970.000000                      2.000000   \n",
       "5993      1970.000000                      2.000000   \n",
       "5994      1960.000000                      0.000000   \n",
       "5995      1970.000000                      2.000000   \n",
       "5996      1975.000000                      0.000000   \n",
       "5997      1955.000000                      3.000000   \n",
       "5998      1950.000000                      0.000000   \n",
       "5999      1965.000000                      0.000000   \n",
       "\n",
       "      Number.of.Unsuccessful.Grant.1  \n",
       "0                           0.000000  \n",
       "1                           1.000000  \n",
       "2                           5.000000  \n",
       "3                           1.000000  \n",
       "4                           2.000000  \n",
       "5                           4.000000  \n",
       "6                           0.000000  \n",
       "7                           1.000000  \n",
       "8                           2.097977  \n",
       "9                           1.000000  \n",
       "10                          0.000000  \n",
       "11                          0.000000  \n",
       "12                          4.000000  \n",
       "13                          5.000000  \n",
       "14                          1.000000  \n",
       "15                          2.000000  \n",
       "16                          3.000000  \n",
       "17                          3.000000  \n",
       "18                          3.000000  \n",
       "19                          1.000000  \n",
       "20                          0.000000  \n",
       "21                         14.000000  \n",
       "22                          2.097977  \n",
       "23                          1.000000  \n",
       "24                          2.000000  \n",
       "25                          0.000000  \n",
       "26                          0.000000  \n",
       "27                          4.000000  \n",
       "28                          2.000000  \n",
       "29                          1.000000  \n",
       "...                              ...  \n",
       "5970                        1.000000  \n",
       "5971                        2.000000  \n",
       "5972                        2.000000  \n",
       "5973                        0.000000  \n",
       "5974                        1.000000  \n",
       "5975                        2.097977  \n",
       "5976                        0.000000  \n",
       "5977                        2.097977  \n",
       "5978                        5.000000  \n",
       "5979                        0.000000  \n",
       "5980                        2.000000  \n",
       "5981                        0.000000  \n",
       "5982                        0.000000  \n",
       "5983                        0.000000  \n",
       "5984                        1.000000  \n",
       "5985                        1.000000  \n",
       "5986                        1.000000  \n",
       "5987                        2.097977  \n",
       "5988                        0.000000  \n",
       "5989                        0.000000  \n",
       "5990                        4.000000  \n",
       "5991                        2.000000  \n",
       "5992                        6.000000  \n",
       "5993                        1.000000  \n",
       "5994                        3.000000  \n",
       "5995                        0.000000  \n",
       "5996                        0.000000  \n",
       "5997                        3.000000  \n",
       "5998                        0.000000  \n",
       "5999                        2.000000  \n",
       "\n",
       "[6000 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_real_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование категориальных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущей ячейке мы разделили наш датасет ещё на две части: в одной присутствуют только вещественные признаки, в другой только категориальные. Это понадобится нам для раздельной последующей обработке этих данных, а так же для сравнения качества работы тех или иных методов.\n",
    "\n",
    "Для использования модели регрессии требуется преобразовать категориальные признаки в вещественные. Рассмотрим основной способ преоборазования категориальных признаков в вещественные: one-hot encoding. Его идея заключается в том, что мы преобразуем категориальный признак при помощи бинарного кода: каждой категории ставим в соответствие набор из нулей и единиц.\n",
    "\n",
    "Посмотрим, как данный метод работает на простом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходные данные:\n",
      "\n",
      "      sex nationality\n",
      "0    male    American\n",
      "1  female    European\n",
      "2    male       Asian\n",
      "3  female    European\n",
      "\n",
      "Закодированные данные:\n",
      "\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "\n",
    "categorial_data = pd.DataFrame({'sex': ['male', 'female', 'male', 'female'], \n",
    "                                'nationality': ['American', 'European', 'Asian', 'European']})\n",
    "print('Исходные данные:\\n')\n",
    "print(categorial_data)\n",
    "encoder = DV(sparse = False)\n",
    "encoded_data = encoder.fit_transform(categorial_data.T.to_dict().values())\n",
    "print('\\nЗакодированные данные:\\n')\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в первые три колонки оказалась закодированна информация о стране, а во вторые две - о поле. При этом для совпадающих элементов выборки строки будут полностью совпадать. Также из примера видно, что кодирование признаков сильно увеличивает их количество, но полностью сохраняет информацию, в том числе о наличии пропущенных значений (их наличие просто становится одним из бинарных признаков в преобразованных данных).\n",
    "\n",
    "Теперь применим one-hot encoding к категориальным признакам из исходного датасета. Обратите внимание на общий для всех методов преобработки данных интерфейс. Функция\n",
    "\n",
    "    encoder.fit_transform(X)\n",
    "    \n",
    "позволяет вычислить необходимые параметры преобразования, впоследствии к новым данным можно уже применять функцию\n",
    "\n",
    "    encoder.transform(X)\n",
    "    \n",
    "Очень важно применять одинаковое преобразование как к обучающим, так и тестовым данным, потому что в противном случае вы получите непредсказуемые, и, скорее всего, плохие результаты. В частности, если вы отдельно закодируете обучающую и тестовую выборку, то получите вообще говоря разные коды для одних и тех же признаков, и ваше решение работать не будет.\n",
    "\n",
    "Также параметры многих преобразований (например, рассмотренное ниже масштабирование) нельзя вычислять одновременно на данных из обучения и теста, потому что иначе подсчитанные на тесте метрики качества будут давать смещённые оценки на качество работы алгоритма. Кодирование категориальных признаков не считает на обучающей выборке никаких параметров, поэтому его можно применять сразу к всему датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DV(sparse = False)\n",
    "X_cat_oh = encoder.fit_transform(X_cat.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения метрики качества по результату обучения требуется разделить исходный датасет на обучающую и тестовую выборки.\n",
    "\n",
    "Обращаем внимание на заданный параметр для генератора случайных чисел: random_state. Так как результаты на обучении и тесте будут зависеть от того, как именно вы разделите объекты, то предлагается использовать заранее определённое значение для получение результатов, согласованных с ответами в системе проверки заданий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train_real_zeros, \n",
    " X_test_real_zeros, \n",
    " y_train, y_test) = train_test_split(X_real_zeros, y, \n",
    "                                     test_size=0.3, \n",
    "                                     random_state=0)\n",
    "(X_train_real_mean, \n",
    " X_test_real_mean) = train_test_split(X_real_mean, \n",
    "                                      test_size=0.3, \n",
    "                                      random_state=0)\n",
    "(X_train_cat_oh,\n",
    " X_test_cat_oh) = train_test_split(X_cat_oh, \n",
    "                                   test_size=0.3, \n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#X_train_real_mean.merge(pd.DataFrame(X_train_cat_oh), left_on='RFCD.Percentage.1', right_on=0)[:5]\n",
    "#pd.DataFrame(X_train_cat_oh)\n",
    "print(type(X_train_real_zeros), type(X_train_real_mean), type(X_train_cat_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили первые наборы данных, для которых выполнены оба ограничения логистической регрессии на входные данные. Обучим на них регрессию, используя имеющийся в библиотеке sklearn функционал по подбору гиперпараметров модели\n",
    "    \n",
    "    optimizer = GridSearchCV(estimator, param_grid)\n",
    "\n",
    "где:\n",
    "- estimator - обучающий алгоритм, для которого будет производиться подбор параметров\n",
    "- param_grid - словарь параметров, ключами которого являются строки-названия, которые передаются алгоритму estimator, а значения - набор параметров для перебора\n",
    "\n",
    "Данный класс выполняет кросс-валидацию обучающей выборки для каждого набора параметров и находит те, на которых алгоритм работает лучше всего. Этот метод позволяет настраивать гиперпараметры по обучающей выборке, избегая переобучения. Некоторые опциональные параметры вызова данного класса, которые нам понадобятся:\n",
    "- scoring - функционал качества, максимум которого ищется кросс валидацией, по умолчанию используется функция score() класса esimator\n",
    "- n_jobs - позволяет ускорить кросс-валидацию, выполняя её параллельно, число определяет количество одновременно запущенных задач\n",
    "- cv - количество фолдов, на которые разбивается выборка при кросс-валидации\n",
    "\n",
    "После инициализации класса GridSearchCV, процесс подбора параметров запускается следующим методом:\n",
    "\n",
    "    optimizer.fit(X, y)\n",
    "    \n",
    "На выходе для получения предсказаний можно пользоваться функцией\n",
    "\n",
    "    optimizer.predict(X)\n",
    "    \n",
    "для меток или\n",
    "\n",
    "    optimizer.predict_proba(X)\n",
    "    \n",
    "для вероятностей (в случае использования логистической регрессии).\n",
    "    \n",
    "Также можно напрямую получить оптимальный класс estimator и оптимальные параметры, так как они является атрибутами класса GridSearchCV:\n",
    "- best\\_estimator\\_ - лучший алгоритм\n",
    "- best\\_params\\_ - лучший набор параметров\n",
    "\n",
    "Класс логистической регрессии выглядит следующим образом:\n",
    "\n",
    "    estimator = LogisticRegression(penalty)\n",
    "   \n",
    "где penalty принимает либо значение 'l2', либо 'l1'. По умолчанию устанавливается значение 'l2', и везде в задании, если об этом не оговорено особо, предполагается использование логистической регрессии с L2-регуляризацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Сравнение способов заполнения вещественных пропущенных значений.\n",
    "1. Составьте две обучающие выборки из вещественных и категориальных признаков: в одной вещественные признаки, где пропущенные значения заполнены нулями, в другой - средними. Рекомендуется записывать в выборки сначала вещественные, а потом категориальные признаки.\n",
    "2. Обучите на них логистическую регрессию, подбирая параметры из заданной сетки param_grid по методу кросс-валидации с числом фолдов cv=3. В качестве оптимизируемой функции используйте заданную по умолчанию.\n",
    "3. Постройте два графика оценок точности +- их стандратного отклонения в зависимости от гиперпараметра и убедитесь, что вы действительно нашли её максимум. Также обратите внимание на большую дисперсию получаемых оценок (уменьшить её можно увеличением числа фолдов cv).\n",
    "4. Получите две метрики качества AUC ROC на тестовой выборке и сравните их между собой. Какой способ заполнения пропущенных вещественных значений работает лучше? В дальнейшем для выполнения задания в качестве вещественных признаков используйте ту выборку, которая даёт лучшее качество на тесте.\n",
    "5. Передайте два значения AUC ROC (сначала для выборки, заполненной средними, потом для выборки, заполненной нулями) в функцию write_answer_1 и запустите её. Полученный файл является ответом на 1 задание.\n",
    "\n",
    "Информация для интересующихся: вообще говоря, не вполне логично оптимизировать на кросс-валидации заданный по умолчанию в классе логистической регрессии функционал accuracy, а измерять на тесте AUC ROC, но это, как и ограничение размера выборки, сделано для ускорения работы процесса кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_scores(optimizer):\n",
    "    scores = [[item[0]['C'], \n",
    "               item[1], \n",
    "               (np.sum((item[2]-item[1])**2)/(item[2].size-1))**0.5] for item in optimizer.grid_scores_]\n",
    "    scores = np.array(scores)\n",
    "    plt.semilogx(scores[:,0], scores[:,1])\n",
    "    plt.fill_between(scores[:,0], scores[:,1]-scores[:,2], \n",
    "                                  scores[:,1]+scores[:,2], alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "def write_answer_1(auc_1, auc_2):\n",
    "    auc = (auc_1 + auc_2)/2\n",
    "    with open(\"preprocessing_lr_answer1.txt\", \"w\") as fout:\n",
    "        fout.write(str(auc))\n",
    "        \n",
    "param_grid = {'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n",
    "cv = 3\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "optimizer = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean = np.hstack((X_train_real_mean, X_train_cat_oh))\n",
    "optimizer.fit(X_train_mean, np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "{'C': 0.05}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.best_estimator_)\n",
    "print(optimizer.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_scores(optimizer):\n",
    "    param_C = [row['C'] for row in optimizer.cv_results_['params']]\n",
    "    test_score = optimizer.cv_results_['mean_test_score']\n",
    "    std_test_score = optimizer.cv_results_['std_test_score']\n",
    "    plt.fill_between(param_C, test_score-std_test_score, test_score+std_test_score, alpha=0.3)\n",
    "    plt.semilogx(param_C, test_score)    \n",
    "    plt.show()\n",
    "\n",
    "plot_scores(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885515825631596\n"
     ]
    }
   ],
   "source": [
    "X_test_mean = np.hstack((X_test_real_mean, X_test_cat_oh))\n",
    "mean_predict = optimizer.predict_proba(X_test_mean)\n",
    "auc1 = roc_auc_score(y_test, mean_predict[:, 1])\n",
    "print(auc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4212e4e79000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer_zeros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train_zeros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_real_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_cat_oh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moptimizer_zeros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_zeros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    215\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator_zeros = LogisticRegression()\n",
    "optimizer_zeros = GridSearchCV(estimator=estimator_zeros, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "X_train_zeros = np.hstack((X_train_real_zeros, X_train_cat_oh))\n",
    "optimizer_zeros.fit(X_train_zeros, np.array(y_train))\n",
    "\n",
    "print(optimizer_zeros.best_estimator_)\n",
    "print(optimizer_zeros.best_params_)\n",
    "\n",
    "plot_scores(optimizer_zeros)\n",
    "\n",
    "X_test_zeros = np.hstack((X_test_real_zeros, X_test_cat_oh))\n",
    "zeros_predict = optimizer_zeros.predict_proba(X_test_zeros)\n",
    "auc2 = roc_auc_score(y_test, zeros_predict[:, 1])\n",
    "print(auc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer_1(auc1, auc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Масштабирование вещественных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем как-то улучшить качество классификации. Для этого посмотрим на сами данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "data_numeric = pd.DataFrame(X_train_real_zeros, columns=numeric_cols)\n",
    "list_cols = ['Number.of.Successful.Grant.1', 'SEO.Percentage.2', 'Year.of.Birth.1']\n",
    "scatter_matrix(data_numeric[list_cols], alpha=0.5, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графиков, разные признаки очень сильно отличаются друг от друга по модулю значений (обратите внимание на диапазоны значений осей x и y). В случае обычной регрессии это никак не влияет на качество обучаемой модели, т.к. у меньших по модулю признаков будут большие веса, но при использовании регуляризации, которая штрафует модель за большие веса, регрессия, как правило, начинает работать хуже.\n",
    "\n",
    "В таких случаях всегда рекомендуется делать стандартизацию (масштабирование) признаков, для того чтобы они меньше отличались друг друга по модулю, но при этом не нарушались никакие другие свойства признакового пространства. При этом даже если итоговое качество модели на тесте уменьшается, это повышает её интерпретабельность, потому что новые веса имеют смысл \"значимости\" данного признака для итоговой классификации.\n",
    "\n",
    "Стандартизация осуществляется посредством вычета из каждого признака среднего значения и нормировки на выборочное стандартное отклонение:\n",
    "\n",
    "$$ x^{scaled}_{id} = \\dfrac{x_{id} - \\mu_d}{\\sigma_d}, \\quad \\mu_d = \\frac{1}{N} \\sum_{i=1}^l x_{id}, \\quad \\sigma_d = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^l (x_{id} - \\mu_d)^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.5. Масштабирование вещественных признаков.\n",
    "\n",
    "1. По аналогии с вызовом one-hot encoder примените масштабирование вещественных признаков для обучающих и тестовых выборок X_train_real_zeros и X_test_real_zeros, используя класс \n",
    "\n",
    "        StandardScaler\n",
    "   \n",
    "   и методы \n",
    "\n",
    "        StandardScaler.fit_transform(...)\n",
    "        StandardScaler.transform(...)\n",
    "2. Сохраните ответ в переменные X_train_real_scaled и X_test_real_scaled соответственно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_real_scaled = scaler.fit_transform(X_train_real_zeros)\n",
    "X_test_real_scaled = scaler.transform(X_test_real_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение признаковых пространств."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим такие же графики для преобразованных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric_scaled = pd.DataFrame(X_train_real_scaled, columns=numeric_cols)\n",
    "list_cols = ['Number.of.Successful.Grant.1', 'SEO.Percentage.2', 'Year.of.Birth.1']\n",
    "scatter_matrix(data_numeric_scaled[list_cols], alpha=0.5, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графиков, мы не поменяли свойства признакового пространства: гистограммы распределений значений признаков, как и их scatter-plots, выглядят так же, как и до нормировки, но при этом все значения теперь находятся примерно в одном диапазоне, тем самым повышая интерпретабельность результатов, а также лучше сочетаясь с идеологией регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Сравнение качества классификации до и после масштабирования вещественных признаков.\n",
    "1. Обучите ещё раз регрессию и гиперпараметры на новых признаках, объединив их с закодированными категориальными.\n",
    "2. Проверьте, был ли найден оптимум accuracy по гиперпараметрам во время кроссвалидации.\n",
    "3. Получите значение ROC AUC на тестовой выборке, сравните с лучшим результатом, полученными ранее.\n",
    "4. Запишите полученный ответ в файл при помощи функции write_answer_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del estimator, optimizer, X_train_mean, X_train_real_mean, X_test_mean, mean_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ea7ecf07779b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mestimator_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_real_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_cat_oh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0moptimizer_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def write_answer_2(auc):\n",
    "    with open(\"preprocessing_lr_answer2.txt\", \"w\") as fout:\n",
    "        fout.write(str(auc))\n",
    "        \n",
    "estimator_scaled = LogisticRegression()\n",
    "optimizer_scaled = GridSearchCV(estimator=estimator_scaled, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "X_train_scaled = np.hstack((X_train_real_scaled, X_train_cat_oh))\n",
    "optimizer_scaled.fit(X_train_scaled, np.array(y_train))\n",
    "\n",
    "print(optimizer_scaled.best_estimator_)\n",
    "print(optimizer_scaled.best_params_)\n",
    "\n",
    "plot_scores(optimizer_scaled)\n",
    "\n",
    "X_test_scaled = np.hstack((X_test_real_scaled, X_test_cat_oh))\n",
    "scaled_predict = optimizer_scaled.predict_proba(X_test_scaled)\n",
    "auc = roc_auc_score(y_test, scaled_predict[:, 1])\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer_2(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Балансировка классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы классификации могут быть очень чувствительны к несбалансированным классам. Рассмотрим пример с выборками, сэмплированными из двух гауссиан. Их мат. ожидания и матрицы ковариации заданы так, что истинная разделяющая поверхность должна проходить параллельно оси x. Поместим в обучающую выборку 20 объектов, сэмплированных из 1-й гауссианы, и 10 объектов из 2-й. После этого обучим на них линейную регрессию, и построим на графиках объекты и области классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\"\"\"Сэмплируем данные из первой гауссианы\"\"\"\n",
    "data_0 = np.random.multivariate_normal([0,0], [[0.5,0],[0,0.5]], size=40)\n",
    "\"\"\"И из второй\"\"\"\n",
    "data_1 = np.random.multivariate_normal([0,1], [[0.5,0],[0,0.5]], size=40)\n",
    "\"\"\"На обучение берём 20 объектов из первого класса и 10 из второго\"\"\"\n",
    "example_data_train = np.vstack([data_0[:20,:], data_1[:10,:]])\n",
    "example_labels_train = np.concatenate([np.zeros((20)), np.ones((10))])\n",
    "\"\"\"На тест - 20 из первого и 30 из второго\"\"\"\n",
    "example_data_test = np.vstack([data_0[20:,:], data_1[10:,:]])\n",
    "example_labels_test = np.concatenate([np.zeros((20)), np.ones((30))])\n",
    "\"\"\"Задаём координатную сетку, на которой будем вычислять область классификации\"\"\"\n",
    "xx, yy = np.meshgrid(np.arange(-3, 3, 0.02), np.arange(-3, 3, 0.02))\n",
    "\"\"\"Обучаем регрессию без балансировки по классам\"\"\"\n",
    "optimizer = GridSearchCV(LogisticRegression(), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train, example_labels_train)\n",
    "\"\"\"Строим предсказания регрессии для сетки\"\"\"\n",
    "Z = optimizer.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "\"\"\"Считаем AUC\"\"\"\n",
    "auc_wo_class_weights = roc_auc_score(example_labels_test, optimizer.predict_proba(example_data_test)[:,1])\n",
    "plt.title('Without class weights')\n",
    "plt.show()\n",
    "print('AUC: %f'%auc_wo_class_weights)\n",
    "\"\"\"Для второй регрессии в LogisticRegression передаём параметр class_weight='balanced'\"\"\"\n",
    "optimizer = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train, example_labels_train)\n",
    "Z = optimizer.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "auc_w_class_weights = roc_auc_score(example_labels_test, optimizer.predict_proba(example_data_test)[:,1])\n",
    "plt.title('With class weights')\n",
    "plt.show()\n",
    "print('AUC: %f'%auc_w_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, во втором случае классификатор находит разделяющую поверхность, которая ближе к истинной, т.е. меньше переобучается. Поэтому на сбалансированность классов в обучающей выборке всегда следует обращать внимание.\n",
    "\n",
    "Посмотрим, сбалансированны ли классы в нашей обучающей выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y_train==0))\n",
    "print(np.sum(y_train==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что нет.\n",
    "\n",
    "Исправить ситуацию можно разными способами, мы рассмотрим два:\n",
    "- давать объектам миноритарного класса больший вес при обучении классификатора (рассмотрен в примере выше)\n",
    "- досэмплировать объекты миноритарного класса, пока число объектов в обоих классах не сравняется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Балансировка классов.\n",
    "1. Обучите логистическую регрессию и гиперпараметры с балансировкой классов, используя веса (параметр class_weight='balanced' регрессии) на отмасштабированных выборках, полученных в предыдущем задании. Убедитесь, что вы нашли максимум accuracy по гиперпараметрам.\n",
    "2. Получите метрику ROC AUC на тестовой выборке.\n",
    "3. Сбалансируйте выборку, досэмплировав в неё объекты из меньшего класса. Для получения индексов объектов, которые требуется добавить в обучающую выборку, используйте следующую комбинацию вызовов функций:\n",
    "        np.random.seed(0)\n",
    "        indices_to_add = np.random.randint(...)\n",
    "        X_train_to_add = X_train[y_train.as_matrix() == 1,:][indices_to_add,:]\n",
    "   После этого добавьте эти объекты в начало или конец обучающей выборки. Дополните соответствующим      образом вектор ответов.\n",
    "4. Получите метрику ROC AUC на тестовой выборке, сравните с предыдущим результатом.\n",
    "5. Внесите ответы в выходной файл при помощи функции write_asnwer_3, передав в неё сначала ROC AUC для балансировки весами, а потом балансировки выборки вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimator_zeros' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3a6f372fcad8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mestimator_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_real_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mestimator_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_scaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'estimator_zeros' is not defined"
     ]
    }
   ],
   "source": [
    "del estimator_zeros, optimizer_zeros, X_train_zeros, X_train_real_zeros, X_test_zeros, zeros_predict\n",
    "del estimator_scaled, optimizer_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "{'C': 0.1}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8871875429220529\n"
     ]
    }
   ],
   "source": [
    "def write_answer_3(auc_1, auc_2):\n",
    "    auc = (auc_1 + auc_2) / 2\n",
    "    with open(\"preprocessing_lr_answer3.txt\", \"w\") as fout:\n",
    "        fout.write(str(auc))\n",
    "        \n",
    "estimator_scaled = LogisticRegression(class_weight='balanced')\n",
    "optimizer_scaled = GridSearchCV(estimator=estimator_scaled, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "X_train_scaled = np.hstack((X_train_real_scaled, X_train_cat_oh))\n",
    "optimizer_scaled.fit(X_train_scaled, np.array(y_train))\n",
    "\n",
    "print(optimizer_scaled.best_estimator_)\n",
    "print(optimizer_scaled.best_params_)\n",
    "\n",
    "plot_scores(optimizer_scaled)\n",
    "\n",
    "X_test_scaled = np.hstack((X_test_real_scaled, X_test_cat_oh))\n",
    "scaled_predict = optimizer_scaled.predict_proba(X_test_scaled)\n",
    "auc1 = roc_auc_score(y_test, scaled_predict[:, 1])\n",
    "print(auc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1884, 5606)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled[y_train.as_matrix() == 1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\series.py:842: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "indices_to_add = np.random.randint(1884, size=432)\n",
    "X_train_to_add = X_train_scaled[y_train.as_matrix() == 1,:][indices_to_add,:]\n",
    "y_train_to_add = y_train[indices_to_add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-c750ca96456f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mestimator_to_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer_to_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator_to_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train_to_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_to_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_train_to_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_to_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_to_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_to_add\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikti\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n\u001b[0;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator_to_add = LogisticRegression()\n",
    "optimizer_to_add = GridSearchCV(estimator=estimator_to_add, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "X_train_to_add = np.vstack((X_train_to_add, X_train_scaled))\n",
    "y_train_to_add = np.vstack((y_train_to_add, y_train))\n",
    "optimizer_scaled.fit(X_train_to_add, np.array(y_train_to_add))\n",
    "\n",
    "print(optimizer_to_add.best_estimator_)\n",
    "print(optimizer_to_add.best_params_)\n",
    "\n",
    "plot_scores(optimizer_to_add)\n",
    "\n",
    "X_test_to_add = np.hstack((X_test_real_scaled, X_test_cat_oh))\n",
    "to_add_predict = optimizer_to_add.predict_proba(X_test_to_add)\n",
    "auc2 = roc_auc_score(y_test, scaled_predict[:, 1])\n",
    "print(auc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стратификация выборок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим ещё раз пример с выборками из нормальных распределений. Посмотрим ещё раз на качество классификаторов, получаемое на тестовых выборках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_wo_class_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-6d3d14829b21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AUC ROC for classifier without weighted classes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_wo_class_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AUC ROC for classifier with weighted classes: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc_w_class_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auc_wo_class_weights' is not defined"
     ]
    }
   ],
   "source": [
    "print('AUC ROC for classifier without weighted classes', auc_wo_class_weights)\n",
    "print('AUC ROC for classifier with weighted classes: ', auc_w_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Насколько эти цифры реально отражают качество работы алгоритма, если учесть, что тестовая выборка так же несбалансирована, как обучающая? При этом мы уже знаем, что алгоритм логистический регрессии чувствителен к балансировке классов в обучающей выборке, т.е. в данном случае на тесте он будет давать заведомо заниженные результаты. Метрика классификатора на тесте имела бы гораздо больший смысл, если бы объекты были разделы в выборках поровну: по 20 из каждого класса на обучени и на тесте. Переформируем выборки и подсчитаем новые ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Разделим данные по классам поровну между обучающей и тестовой выборками\"\"\"\n",
    "example_data_train = np.vstack([data_0[:20,:], data_1[:20,:]])\n",
    "example_labels_train = np.concatenate([np.zeros((20)), np.ones((20))])\n",
    "example_data_test = np.vstack([data_0[20:,:], data_1[20:,:]])\n",
    "example_labels_test = np.concatenate([np.zeros((20)), np.ones((20))])\n",
    "\"\"\"Обучим классификатор\"\"\"\n",
    "optimizer = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train, example_labels_train)\n",
    "Z = optimizer.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "auc_stratified = roc_auc_score(example_labels_test, optimizer.predict_proba(example_data_test)[:,1])\n",
    "plt.title('With class weights')\n",
    "plt.show()\n",
    "print('AUC ROC for stratified samples: ', auc_stratified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, после данной процедуры ответ классификатора изменился незначительно, а вот качество увеличилось. При этом, в зависимости от того, как вы разбили изначально данные на обучение и тест, после сбалансированного разделения выборок итоговая метрика на тесте может как увеличиться, так и уменьшиться, но доверять ей можно значительно больше, т.к. она построена с учётом специфики работы классификатора. Данный подход является частным случаем т.н. метода стратификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Стратификация выборки.\n",
    "\n",
    "1. По аналогии с тем, как это было сделано в начале задания, разбейте выборки X_real_zeros и X_cat_oh на обучение и тест, передавая в функцию \n",
    "        train_test_split(...)\n",
    "   дополнительно параметр \n",
    "       stratify=y\n",
    "   Также обязательно передайте в функцию переменную random_state=0.\n",
    "2. Выполните масштабирование новых вещественных выборок, обучите классификатор и его гиперпараметры при помощи метода кросс-валидации, делая поправку на несбалансированные классы при помощи весов. Убедитесь в том, что нашли оптимум accuracy по гиперпараметрам.\n",
    "3. Оцените качество классификатора метрике AUC ROC на тестовой выборке.\n",
    "4. Полученный ответ передайте функции write_answer_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_answer_4(auc):\n",
    "    with open(\"preprocessing_lr_answer4.txt\", \"w\") as fout:\n",
    "        fout.write(str(auc))\n",
    "        \n",
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы разобрались с основными этапами предобработки данных для линейных классификаторов.\n",
    "Напомним основные этапы:\n",
    "- обработка пропущенных значений\n",
    "- обработка категориальных признаков\n",
    "- стратификация\n",
    "- балансировка классов\n",
    "- масштабирование\n",
    "\n",
    "Данные действия с данными рекомендуется проводить всякий раз, когда вы планируете использовать линейные методы. Рекомендация по выполнению многих из этих пунктов справедлива и для других методов машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трансформация признаков.\n",
    "\n",
    "Теперь рассмотрим способы преобразования признаков. Существует достаточно много различных способов трансформации признаков, которые позволяют при помощи линейных методов получать более сложные разделяющие поверхности. Самым базовым является полиномиальное преобразование признаков. Его идея заключается в том, что помимо самих признаков вы дополнительно включаете набор все полиномы степени $p$, которые можно из них построить. Для случая $p=2$ преобразование выглядит следующим образом:\n",
    "\n",
    "$$ \\phi(x_i) = [x_{i,1}^2, ..., x_{i,D}^2, x_{i,1}x_{i,2}, ..., x_{i,D} x_{i,D-1}, x_{i,1}, ..., x_{i,D}, 1] $$\n",
    "\n",
    "Рассмотрим принцип работы данных признаков на данных, сэмплированных их гауссиан:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9da7e19ef5bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;34m\"\"\"Обучаем преобразование на обучающей выборке, применяем его к тестовой\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mexample_data_train_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_data_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mexample_data_test_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_data_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;34m\"\"\"Обращаем внимание на параметр fit_intercept=False\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_data_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\"\"\"Инициализируем класс, который выполняет преобразование\"\"\"\n",
    "transform = PolynomialFeatures(2)\n",
    "\"\"\"Обучаем преобразование на обучающей выборке, применяем его к тестовой\"\"\"\n",
    "example_data_train_poly = transform.fit_transform(example_data_train)\n",
    "example_data_test_poly = transform.transform(example_data_test)\n",
    "\"\"\"Обращаем внимание на параметр fit_intercept=False\"\"\"\n",
    "optimizer = GridSearchCV(LogisticRegression(class_weight='balanced', fit_intercept=False), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train_poly, example_labels_train)\n",
    "Z = optimizer.predict(transform.transform(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "plt.title('With class weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что данный метод преобразования данных уже позволяет строить нелинейные разделяющие поверхности, которые могут более тонко подстраиваться под данные и находить более сложные зависимости. Число признаков в новой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_data_train_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но при этом одновременно данный метод способствует более сильной способности модели к переобучению из-за быстрого роста числа признаком с увеличением степени $p$. Рассмотрим пример с $p=11$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PolynomialFeatures(11)\n",
    "example_data_train_poly = transform.fit_transform(example_data_train)\n",
    "example_data_test_poly = transform.transform(example_data_test)\n",
    "optimizer = GridSearchCV(LogisticRegression(class_weight='balanced', fit_intercept=False), param_grid, cv=cv, n_jobs=-1)\n",
    "optimizer.fit(example_data_train_poly, example_labels_train)\n",
    "Z = optimizer.predict(transform.transform(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "plt.scatter(data_0[:,0], data_0[:,1], color='red')\n",
    "plt.scatter(data_1[:,0], data_1[:,1], color='blue')\n",
    "plt.title('Corrected class weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество признаков в данной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_data_train_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Трансформация вещественных признаков.\n",
    "\n",
    "1. Реализуйте по аналогии с примером преобразование вещественных признаков модели при помощи полиномиальных признаков степени 2\n",
    "2. Постройте логистическую регрессию на новых данных, одновременно подобрав оптимальные гиперпараметры. Обращаем внимание, что в преобразованных признаках уже присутствует столбец, все значения которого равны 1, поэтому обучать дополнительно значение $b$ не нужно, его функцию выполняет один из весов $w$. В связи с этим во избежание линейной зависимости в датасете, в вызов класса логистической регрессии требуется передавать параметр fit_intercept=False. Для обучения используйте стратифицированные выборки с балансировкой классов при помощи весов, преобразованные признаки требуется заново отмасштабировать.\n",
    "3. Получите AUC ROC на тесте и сравните данный результат с использованием обычных признаков.\n",
    "4. Передайте полученный ответ в функцию write_answer_5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_answer_5(auc):\n",
    "    with open(\"preprocessing_lr_answer5.txt\", \"w\") as fout:\n",
    "        fout.write(str(auc))\n",
    "        \n",
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия Lasso.\n",
    "К логистической регрессии также можно применить L1-регуляризацию (Lasso), вместо регуляризации L2, которая будет приводить к отбору признаков. Вам предлагается применить L1-регуляцию к исходным признакам и проинтерпретировать полученные результаты (применение отбора признаков к полиномиальным так же можно успешно применять, но в нём уже будет отсутствовать компонента интерпретации, т.к. смысловое значение оригинальных признаков известно, а полиномиальных - уже может быть достаточно нетривиально). Для вызова логистической регрессии с L1-регуляризацией достаточно передать параметр penalty='l1' в инициализацию класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Отбор признаков при помощи регрессии Lasso.\n",
    "1. Обучите регрессию Lasso на стратифицированных отмасштабированных выборках, используя балансировку классов при помощи весов.\n",
    "2. Получите ROC AUC регрессии, сравните его с предыдущими результатами.\n",
    "3. Найдите номера вещественных признаков, которые имеют нулевые веса в итоговой модели.\n",
    "4. Передайте их список функции write_answer_6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_answer_6(features):\n",
    "    with open(\"preprocessing_lr_answer6.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(num) for num in features]))\n",
    "        \n",
    "# place your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
